{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQIF0jgMi_bZ"
      },
      "source": [
        "# Aula 10\n",
        "\n",
        "# Trabalhando com o PySpark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8cxF4E6Skr"
      },
      "source": [
        "### Introdução\n",
        "\n",
        "\n",
        "Usaremos o conjunto de dados relacionado a campanhas de marketing direto (chamadas telefônicas) de uma instituição bancária. O objetivo da classificação é prever se o cliente irá realizar (Sim/Não) um depósito a prazo. O conjunto de dados pode ser baixado no link [aqui](https://archive.ics.uci.edu/dataset/222/bank+marketing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w3cYF85Gi_bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19M95VTZyZ5HzBxlIALMr1Qyo5TbMc1Jy\n",
            "To: e:\\Senac\\Aulas\\CursoBigData\\Semana10\\file.csv\n",
            "100%|██████████| 919k/919k [00:00<00:00, 2.34MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# URL do Google Drive\n",
        "url = 'https://drive.google.com/uc?id=19M95VTZyZ5HzBxlIALMr1Qyo5TbMc1Jy'\n",
        "\n",
        "# Baixando o arquivo\n",
        "output = 'file.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Inicializando o SparkSession\n",
        "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
        "\n",
        "# Lendo o arquivo CSV baixado\n",
        "df = spark.read.csv(output, header=True, inferSchema=True)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mas podemos converte em um DataFrame do PySpark para um DataFrame do Pandas. O método `df.take(5)` obtém as primeiras 5 linhas do DataFrame do PySpark `df`, e a função `pd.DataFrame(..., columns=df.columns)` converte essas 5 linhas em um DataFrame do Pandas, preservando os nomes das colunas originais do DataFrame do PySpark. O resultado é um DataFrame do Pandas contendo as primeiras 5 linhas do DataFrame original do PySpark, facilitando assim a visualização e análise rápida dos dados em um ambiente local utilizando as funcionalidades do Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbTbybrOi_bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O código a seguir agrupa os dados pela coluna `'deposit'`, contabilizando quantos clientes realizaram ('Yes') e quantos não realizaram ('No') um depósito a prazo. Em seguida, converte o resultado para um DataFrame do Pandas, facilitando a manipulação e visualização dos dados. O resultado final é uma tabela que mostra a contagem de registros para cada valor na coluna `'deposit'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pYvQlH6i_bd",
        "outputId": "e8248ab6-3a8f-4414-a432-bc8e93211e29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, vamos uma lista chamada `numeric_features`, que contém os nomes das colunas do DataFrame `df` que têm o tipo de dado inteiro. Em seguida, selecionamos essas colunas do DataFrame e aplicamos o método `describe()` para obter estatísticas descritivas, como média, desvio padrão, valores mínimo e máximo. Por fim, convertemos o resultado para um DataFrame do Pandas e transpomos a tabela para facilitar a visualização dessas estatísticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f5ENqTJi_bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A principal conclusão da matriz de correlação apresentada é que a maioria das variáveis numéricas não possuem fortes correlações entre si. No entanto, há uma correlação moderadamente forte entre as variáveis `pdays` e `previous` (0.507272). Isso indica que há uma relação significativa entre o número de dias desde que um cliente foi contatado por uma campanha anterior (`pdays`) e o número de contatos realizados antes dessa campanha (`previous`). Em geral, as outras correlações são bastante fracas, sugerindo que as variáveis numéricas selecionadas são relativamente independentes umas das outras no contexto deste conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yy4_P-Wi_bd"
      },
      "source": [
        "Gerando a matriz de dispersão:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3v18pYli_bd",
        "outputId": "d4c801b3-d4a7-4fb9-c7d7-b7183ef7b774"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "givOOxpzaT8w"
      },
      "source": [
        "### Usando os dados para fazer previsão de cenários futuros\n",
        "\n",
        "Vamos realizar a indexação e codificação de colunas categóricas de um DataFrame do PySpark. Primeiro, vamos importar as bibliotecas necessárias e define uma lista de colunas categóricas. Para cada coluna, vamos criar um `StringIndexer` que converte os valores categóricos em índices numéricos. Em seguida, usa um `OneHotEncoderEstimator` para transformar esses índices em vetores binários one-hot. Esses transformadores são armazenados em uma lista chamada `stages`, que será usada em um pipeline para aplicar essas transformações aos dados. O resultado final é um DataFrame com as colunas categóricas convertidas em vetores binários, facilitando a análise e o uso em algoritmos de aprendizado de máquina."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ORg4l3x7axCu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StringIndexer_542057a63ce1\n",
            "OneHotEncoder_29d0321f31bf\n",
            "StringIndexer_d0ee5d93867a\n",
            "OneHotEncoder_235f6c8f55b0\n",
            "StringIndexer_110e000c7105\n",
            "OneHotEncoder_d6d417d6ffae\n",
            "StringIndexer_8dd09672383f\n",
            "OneHotEncoder_b6fe1db8d2a4\n",
            "StringIndexer_9896f3b6c173\n",
            "OneHotEncoder_b491d0309705\n",
            "StringIndexer_3b3f56df9f05\n",
            "OneHotEncoder_c2ba1dce4035\n",
            "StringIndexer_4e94cffbf0ef\n",
            "OneHotEncoder_ae1435ab4f8c\n",
            "StringIndexer_df86b9d65d00\n",
            "OneHotEncoder_d69607603f0c\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "\n",
        "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "\n",
        "stages = []\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + 'Index')\n",
        "    encoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=categoricalCol + 'classVec')\n",
        "    stages += [stringIndexer, encoder]\n",
        "\n",
        "# Visualização do estágio\n",
        "for stage in stages:\n",
        "    print(stage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos o StringIndexer novamente para codificar nossas etiquetas em índices de etiquetas. Em seguida, usamos o VectorAssembler para combinar todas as colunas de características em uma única coluna de vetor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MIJV7m5b4J6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnZ1Bh19cG8t"
      },
      "source": [
        "Pipeline\n",
        "\n",
        "Usamos o Pipeline para encadear múltiplos Transformadores e Estimadores juntos para especificar nosso fluxo de trabalho de aprendizado de máquina. Os estágios de um Pipeline são especificados como um array ordenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAsmoeC9cSEZ",
        "outputId": "c96b01d4-dafc-4017-e1de-b7ac096bcb2f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xLjN_iNNcpcc",
        "outputId": "41491533-7d6c-4ce9-a563-5f687e6bc5c3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Separando o conjunto em treino e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "7_ehaWnqdh3s",
        "outputId": "5364a779-ef96-4f67-c83f-bf03f4571387"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criando um Modelo de Regressão Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V4mscsyPd4Or",
        "outputId": "f8c28a67-0e18-493b-85e8-7105d33664df"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em um modelo de regressão logística, os gráficos \"Beta Coefficients\", \"ROC Curve\" e \"Precision x Recall\" servem para diferentes propósitos de análise e interpretação do desempenho do modelo:\n",
        "\n",
        "1. **Beta Coefficients**:\n",
        "   - **Propósito**: Este gráfico mostra os coeficientes estimados para cada variável independente (ou característica) no modelo de regressão logística.\n",
        "   - **Interpretação**: Os coeficientes indicam a direção e a magnitude da associação entre as variáveis independentes e a variável dependente. Coeficientes positivos indicam que o aumento na variável independente está associado a um aumento na probabilidade do resultado, enquanto coeficientes negativos indicam o oposto.\n",
        "\n",
        "2. **ROC Curve (Receiver Operating Characteristic Curve)**:\n",
        "   - **Propósito**: Este gráfico é usado para avaliar a capacidade do modelo em distinguir entre as classes (por exemplo, positivo vs. negativo).\n",
        "   - **Interpretação**: A curva ROC plota a taxa de verdadeiros positivos (sensibilidade) contra a taxa de falsos positivos (1 - especificidade) para diferentes limiares de classificação. A área sob a curva ROC (AUC) é uma medida do desempenho do modelo; quanto mais próximo de 1, melhor a capacidade de discriminação do modelo.\n",
        "\n",
        "3. **Precision x Recall**:\n",
        "   - **Propósito**: Este gráfico é usado para avaliar o equilíbrio entre a precisão (quantos dos exemplos classificados como positivos são realmente positivos) e a recall (quantos dos exemplos realmente positivos são classificados corretamente pelo modelo).\n",
        "   - **Interpretação**: A curva de Precisão x Recall é especialmente útil em cenários de classes desbalanceadas, onde uma classe é muito mais frequente do que a outra. A análise desse gráfico ajuda a entender o trade-off entre precisão e recall e a escolher o limiar de decisão mais adequado para o problema em questão.\n",
        "\n",
        "Em resumo:\n",
        "- **Beta Coefficients**: Indicam a influência das variáveis independentes no modelo.\n",
        "- **ROC Curve**: Avalia a capacidade do modelo de distinguir entre classes.\n",
        "- **Precision x Recall**: Avalia o equilíbrio entre precisão e recall, especialmente útil para classes desbalanceadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "ddM9X0jXgDnE",
        "outputId": "aa2da9ee-e38e-49a8-84bc-7c1c23892ed8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
